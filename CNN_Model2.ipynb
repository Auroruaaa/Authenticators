{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global parameters (you can adjust these later)\n",
    "T = 160\n",
    "delta = 4\n",
    "Delta = 8\n",
    "Gamma = 30\n",
    "channels = ['Oz..', 'T7..', 'Cz..']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_edf(file_path, channels, T=160, delta=4, Delta=8, Gamma=30):\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
    "    raw.pick(channels)\n",
    "    data = raw.get_data().T.astype(np.float32)\n",
    "    \n",
    "    # Normalize the data.\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data).astype(np.float32)\n",
    "    \n",
    "    n_samples = normalized_data.shape[0]\n",
    "    windows = [normalized_data[start:start+T, :] \n",
    "               for start in range(0, n_samples - T + 1, delta)]\n",
    "    windows = np.array(windows, dtype=np.float32)\n",
    "    \n",
    "    # Create augmented samples by grouping Gamma consecutive windows with a step of Delta.\n",
    "    augmented = [windows[i:i+Gamma] \n",
    "                 for i in range(0, len(windows) - Gamma + 1, Delta)]\n",
    "    augmented = np.array(augmented, dtype=np.float32)\n",
    "    \n",
    "    return augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building index: 100%|██████████| 90/90 [00:03<00:00, 24.74it/s]\n"
     ]
    }
   ],
   "source": [
    "class PhysionetAugmentedDataset(Dataset):\n",
    "    def __init__(self, subject_ids, channels, T=160, delta=4, Delta=8, Gamma=30):\n",
    "        self.subject_ids = subject_ids\n",
    "        self.channels = channels\n",
    "        self.T = T\n",
    "        self.delta = delta\n",
    "        self.Delta = Delta\n",
    "        self.Gamma = Gamma\n",
    "        self.indices = []\n",
    "        \n",
    "        for label, subject in enumerate(tqdm(subject_ids, desc=\"Building index\")):\n",
    "            for r in range(1, 2):\n",
    "                file_path = f'./files/S{subject:03d}/S{subject:03d}R{r:02d}.edf'\n",
    "                try:\n",
    "                    augmented = process_edf(file_path, channels, T, delta, Delta, Gamma)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "                num_augmented = augmented.shape[0]\n",
    "                for aug_idx in range(num_augmented):\n",
    "                    self.indices.append((subject, r, aug_idx, file_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subject, r, aug_idx, file_path, label = self.indices[idx]\n",
    "        augmented = process_edf(file_path, self.channels, self.T, self.delta, self.Delta, self.Gamma)\n",
    "        sample = augmented[aug_idx]\n",
    "        sample_tensor = torch.tensor(sample, dtype=torch.float32)\n",
    "        return sample_tensor, label\n",
    "    \n",
    "subjects = list(range(1, 91))\n",
    "dataset = PhysionetAugmentedDataset(subjects, channels, T, delta, Delta, Gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subject groups:\n",
    "all_indices = list(range(len(dataset)))\n",
    "labels = [dataset.indices[i][-1] for i in all_indices]  # label is the last element in the index tuple\n",
    "\n",
    "# Perform a stratified split (80% train, 20% test)\n",
    "train_idx, test_idx = train_test_split(all_indices, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Create subset datasets using the indices.\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "# Create DataLoaders for both splits.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    _, label = train_dataset[i]\n",
    "    all_labels.append(label)\n",
    "unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
    "print(\"Training label counts:\", dict(zip(unique_labels, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(1,2))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(153600, 1024)\n",
    "        self.dense2 = nn.Linear(1024, 90)\n",
    "        self.dropout = nn.Dropout2d(0.5) #hyperparameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 20  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_train_preds = []\n",
    "    all_train_labels = {}\n",
    "\n",
    "    for batch_data, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_train_preds.extend(preds.cpu().numpy())\n",
    "        all_train_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Train Accuracy = {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_test_preds = []\n",
    "all_test_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        outputs = model(batch_data)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_test_preds.extend(preds.cpu().numpy())\n",
    "        all_test_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = accuracy_score(all_test_labels, all_test_preds)\n",
    "    print(f\"Epoch {epoch+1}: Test Accuracy = {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset): \n",
    "    def __init__(self, base_dataset, similar_prob=0.5, min_time_gap=5):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.similar_prob = similar_prob\n",
    "        self.min_time_gap = min_time_gap\n",
    "\n",
    "        self.subject_file_to_indices = {}\n",
    "        for i, entry in enumerate(base_dataset.indices):\n",
    "            subject = entry[-1]\n",
    "            file_path = entry[3]\n",
    "            key = (subject, file_path)\n",
    "            if key not in self.subject_file_to_indices:\n",
    "                self.subject_file_to_indices[key] = []\n",
    "            self.subject_file_to_indices[key].append(i)\n",
    "            \n",
    "        self.all_indices = list(range(len(base_dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if random.random() < self.similar_prob:\n",
    "            label = 1\n",
    "\n",
    "            index1 = idx\n",
    "            subject, recording, aug_idx, file_path, label = self.base_dataset.indices[index1]\n",
    "            # Only choose candidates that come from the same subject and same file.\n",
    "            key = (subject, file_path)\n",
    "            candidate_indices = self.subject_file_to_indices.get(key, [])\n",
    "\n",
    "            if len(candidate_indices) < 2:\n",
    "                label = 0\n",
    "                index2 = random.choice(self.all_indices)\n",
    "                \n",
    "                while self.base_dataset.indices[index2][-1] == label:\n",
    "                    index2 = random.choice(self.all_indices)\n",
    "\n",
    "            else:\n",
    "                index2 = random.choice(candidate_indices)\n",
    "\n",
    "                while index2 == index1 or (\n",
    "                    abs(self.base_dataset.indices[index1][2] - self.base_dataset.indices[index2][2]) < self.min_time_gap\n",
    "                ):\n",
    "                    index2 = random.choice(candidate_indices)\n",
    "        \n",
    "        else: \n",
    "            label = 0\n",
    "            index1 = idx\n",
    "            subject1 = self.base_dataset.indices[index1][-1]\n",
    "            index2 = random.choice(self.all_indices)\n",
    "            \n",
    "            while self.base_dataset.indices[index2][-1] == subject1:\n",
    "                index2 = random.choice(self.all_indices)\n",
    "\n",
    "        sample1, _ = self.base_dataset[index1]\n",
    "        sample2, _ = self.base_dataset[index2]\n",
    "        return sample1, sample2, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dataset = SiameseDataset(dataset, similar_prob=0.5, min_time_gap=5)\n",
    "siamese_loader = DataLoader(siamese_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 26715\n",
      "Pair label: 0.0\n",
      "Sample 1 shape: torch.Size([30, 160, 3])\n",
      "Sample 2 shape: torch.Size([30, 160, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset length:\", len(siamese_dataset))\n",
    "\n",
    "# Retrieve and inspect the first sample.\n",
    "sample1, sample2, pair_label = siamese_dataset[0]\n",
    "print(\"Pair label:\", pair_label.item())\n",
    "print(\"Sample 1 shape:\", sample1.shape)\n",
    "print(\"Sample 2 shape:\", sample2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
