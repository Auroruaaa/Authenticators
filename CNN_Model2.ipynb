{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global parameters (you can adjust these later)\n",
    "T = 160\n",
    "delta = 4\n",
    "Delta = 8\n",
    "Gamma = 30\n",
    "channels = ['Oz..', 'T7..', 'Cz..']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_edf(file_path, channels, T=160, delta=4, Delta=8, Gamma=30):\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
    "    raw.pick(channels)\n",
    "    data = raw.get_data().T.astype(np.float32)\n",
    "    \n",
    "    # Normalize the data.\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data).astype(np.float32)\n",
    "    \n",
    "    n_samples = normalized_data.shape[0]\n",
    "    windows = [normalized_data[start:start+T, :] \n",
    "               for start in range(0, n_samples - T + 1, delta)]\n",
    "    windows = np.array(windows, dtype=np.float32)\n",
    "    \n",
    "    # Create augmented samples by grouping Gamma consecutive windows with a step of Delta.\n",
    "    augmented = [windows[i:i+Gamma] \n",
    "                 for i in range(0, len(windows) - Gamma + 1, Delta)]\n",
    "    augmented = np.array(augmented, dtype=np.float32)\n",
    "    \n",
    "    return augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysionetAugmentedDataset(Dataset):\n",
    "    def __init__(self, subject_ids, channels, T=160, delta=4, Delta=8, Gamma=30):\n",
    "        self.subject_ids = subject_ids\n",
    "        self.channels = channels\n",
    "        self.T = T\n",
    "        self.delta = delta\n",
    "        self.Delta = Delta\n",
    "        self.Gamma = Gamma\n",
    "        self.indices = []\n",
    "        \n",
    "        for label, subject in enumerate(tqdm(subject_ids, desc=\"Building index\")):\n",
    "            for r in range(1, 15):\n",
    "                file_path = f'./files/S{subject:03d}/S{subject:03d}R{r:02d}.edf'\n",
    "                try:\n",
    "                    augmented = process_edf(file_path, channels, T, delta, Delta, Gamma)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "                num_augmented = augmented.shape[0]\n",
    "                for aug_idx in range(num_augmented):\n",
    "                    self.indices.append((subject, r, aug_idx, file_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subject, r, aug_idx, file_path, label = self.indices[idx]\n",
    "        augmented = process_edf(file_path, self.channels, self.T, self.delta, self.Delta, self.Gamma)\n",
    "        sample = augmented[aug_idx]\n",
    "        sample_tensor = torch.tensor(sample, dtype=torch.float32)\n",
    "        return sample_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subject groups:\n",
    "alpha_subjects = list(range(1, 91))    # Training subjects\n",
    "beta_subjects = list(range(91, 110))     # Test subjects\n",
    "\n",
    "# Create datasets:\n",
    "train_dataset = PhysionetAugmentedDataset(alpha_subjects, channels, T, delta, Delta, Gamma)\n",
    "test_dataset = PhysionetAugmentedDataset(beta_subjects, channels, T, delta, Delta, Gamma)\n",
    "\n",
    "# Create DataLoaders:\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(1,2))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(153600, 1024)\n",
    "        self.dense2 = nn.Linear(1024, 90)\n",
    "        self.dropout = nn.Dropout2d(0.5) #hyperparameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_data, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        outputs = model(batch_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
