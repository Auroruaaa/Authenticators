{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global parameters (you can adjust these later)\n",
    "T = 160\n",
    "delta = 4\n",
    "Delta = 8\n",
    "Gamma = 30\n",
    "channels = ['Oz..', 'T7..', 'Cz..']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_edf(file_path, channels, T=160, delta=4, Delta=8, Gamma=30):\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
    "    raw.pick(channels)\n",
    "    data = raw.get_data().T.astype(np.float32)\n",
    "    \n",
    "    # Normalize the data.\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data).astype(np.float32)\n",
    "    \n",
    "    n_samples = normalized_data.shape[0]\n",
    "    windows = [normalized_data[start:start+T, :] \n",
    "               for start in range(0, n_samples - T + 1, delta)]\n",
    "    windows = np.array(windows, dtype=np.float32)\n",
    "    \n",
    "    # Create augmented samples by grouping Gamma consecutive windows with a step of Delta.\n",
    "    augmented = [windows[i:i+Gamma] \n",
    "                 for i in range(0, len(windows) - Gamma + 1, Delta)]\n",
    "    augmented = np.array(augmented, dtype=np.float32)\n",
    "    \n",
    "    return augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building index: 100%|██████████| 90/90 [00:05<00:00, 17.92it/s]\n"
     ]
    }
   ],
   "source": [
    "class PhysionetAugmentedDataset(Dataset):\n",
    "    def __init__(self, subject_ids, channels, T=160, delta=4, Delta=8, Gamma=30):\n",
    "        self.subject_ids = subject_ids\n",
    "        self.channels = channels\n",
    "        self.T = T\n",
    "        self.delta = delta\n",
    "        self.Delta = Delta\n",
    "        self.Gamma = Gamma\n",
    "        self.indices = []\n",
    "        \n",
    "        for label, subject in enumerate(tqdm(subject_ids, desc=\"Building index\")):\n",
    "            for r in range(1, 3):\n",
    "                file_path = f'./files/S{subject:03d}/S{subject:03d}R{r:02d}.edf'\n",
    "                try:\n",
    "                    augmented = process_edf(file_path, channels, T, delta, Delta, Gamma)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "                num_augmented = augmented.shape[0]\n",
    "                for aug_idx in range(num_augmented):\n",
    "                    self.indices.append((subject, r, aug_idx, file_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subject, r, aug_idx, file_path, label = self.indices[idx]\n",
    "        augmented = process_edf(file_path, self.channels, self.T, self.delta, self.Delta, self.Gamma)\n",
    "        sample = augmented[aug_idx]\n",
    "        sample_tensor = torch.tensor(sample, dtype=torch.float32)\n",
    "        return sample_tensor, label\n",
    "    \n",
    "subjects = list(range(1, 91))\n",
    "dataset = PhysionetAugmentedDataset(subjects, channels, T, delta, Delta, Gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subject groups:\n",
    "all_indices = list(range(len(dataset)))\n",
    "labels = [dataset.indices[i][-1] for i in all_indices]  # label is the last element in the index tuple\n",
    "\n",
    "# Perform a stratified split (80% train, 20% test)\n",
    "train_idx, test_idx = train_test_split(all_indices, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Create subset datasets using the indices.\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "# Create DataLoaders for both splits.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(1,2))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(153600, 1024)\n",
    "        self.dense2 = nn.Linear(1024, 90)\n",
    "        self.dropout = nn.Dropout2d(0.5) #hyperparameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 668/668 [05:10<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 3.1628, Train Accuracy = 0.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 668/668 [05:11<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 1.9437, Train Accuracy = 0.4610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 668/668 [05:13<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 1.5204, Train Accuracy = 0.5532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 668/668 [05:13<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 1.2239, Train Accuracy = 0.6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 668/668 [05:13<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.9922, Train Accuracy = 0.6884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 668/668 [05:13<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 0.8074, Train Accuracy = 0.7420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 668/668 [05:14<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 0.6917, Train Accuracy = 0.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 668/668 [05:17<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss = 0.5694, Train Accuracy = 0.8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 668/668 [05:17<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss = 0.4778, Train Accuracy = 0.8409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 668/668 [05:16<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.4145, Train Accuracy = 0.8607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 668/668 [05:16<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss = 0.3492, Train Accuracy = 0.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 668/668 [05:16<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss = 0.3011, Train Accuracy = 0.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 668/668 [05:12<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss = 0.2642, Train Accuracy = 0.9101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 668/668 [05:13<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss = 0.2415, Train Accuracy = 0.9198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 668/668 [05:13<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss = 0.1945, Train Accuracy = 0.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 668/668 [05:12<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss = 0.1768, Train Accuracy = 0.9415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 668/668 [05:13<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss = 0.1555, Train Accuracy = 0.9497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 668/668 [05:14<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss = 0.1608, Train Accuracy = 0.9479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 668/668 [05:15<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss = 0.1330, Train Accuracy = 0.9550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 668/668 [05:16<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss = 0.1204, Train Accuracy = 0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 20  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_train_preds = []\n",
    "    all_train_labels = []\n",
    "\n",
    "    for batch_data, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_train_preds.extend(preds.cpu().numpy())\n",
    "        all_train_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Train Accuracy = {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 167/167 [01:15<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Test Accuracy = 0.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_test_preds = []\n",
    "all_test_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        outputs = model(batch_data)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_test_preds.extend(preds.cpu().numpy())\n",
    "        all_test_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = accuracy_score(all_test_labels, all_test_preds)\n",
    "    print(f\"Epoch {epoch+1}: Test Accuracy = {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load)\n",
    "model.eval()\n",
    "\n",
    "embedding_output = None\n",
    "def hook_fn(module, input, output):\n",
    "    global embedding_output\n",
    "    embedding_output = output.detach()\n",
    "\n",
    "hook_handle = model.dropout.register_forward_hook(hook_fn)\n",
    "\n",
    "def get_embedding(sample):\n",
    "    global embedding_output\n",
    "    if sample.dim() == 3:\n",
    "        sample = sample.unsqueeze(0)\n",
    "    sample = sample.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(sample)\n",
    "\n",
    "    emb = embedding_output.unsqueeze(0)\n",
    "    norm = torch.norm(emb, p=2)\n",
    "    if norm > 0:\n",
    "        emb = emb / norm\n",
    "    return emb.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrolled_subjects = {\n",
    "    'subject1': [sample1, sample2, sample3],\n",
    "    'subject2': [sample1, sample2, sample3],\n",
    "    # Add additional subjects as needed.\n",
    "}\n",
    "\n",
    "subject_fingerprints = {}\n",
    "for subject_id, samples in enrolled_subjects.items():\n",
    "    embeddings = [get_embedding(sample) for sample in samples]\n",
    "    embeddings = np.stack(embeddings, axis=0)\n",
    "    # Average embeddings across samples to create a fingerprint.\n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    # Normalize the fingerprint.\n",
    "    avg_embedding = avg_embedding / np.linalg.norm(avg_embedding)\n",
    "    subject_fingerprints[subject_id] = avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sample(sample, subject_fingerprints, threshold=0.275):\n",
    "    \"\"\"\n",
    "    Classify a new EEG sample by comparing its embedding to enrolled subject fingerprints.\n",
    "    The classification uses cosine distance with a threshold margin.\n",
    "    \n",
    "    Args:\n",
    "        sample (torch.Tensor): New EEG sample tensor.\n",
    "        subject_fingerprints (dict): Mapping of subject IDs to fingerprint embeddings.\n",
    "        threshold (float): Threshold margin for cosine distance.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predicted_subject, similarity_scores, cosine_distances)\n",
    "               predicted_subject is the subject if the minimum cosine distance is below threshold,\n",
    "               otherwise it is set to 'dissimilar' (or 'unknown').\n",
    "    \"\"\"\n",
    "    emb = get_embedding(sample)\n",
    "    similarities = {}\n",
    "    cosine_distances = {}\n",
    "    \n",
    "    # Compute cosine similarity and derive cosine distance.\n",
    "    for subject_id, fingerprint in subject_fingerprints.items():\n",
    "        similarity = np.dot(emb, fingerprint)\n",
    "        distance = 1 - similarity  # cosine distance for normalized vectors.\n",
    "        similarities[subject_id] = similarity\n",
    "        cosine_distances[subject_id] = distance\n",
    "    \n",
    "    # Identify the subject with the minimum cosine distance.\n",
    "    best_subject = min(cosine_distances, key=cosine_distances.get)\n",
    "    best_distance = cosine_distances[best_subject]\n",
    "    \n",
    "    # If the best distance is less than the threshold, we consider it a match.\n",
    "    if best_distance < threshold:\n",
    "        predicted_subject = best_subject\n",
    "    else:\n",
    "        predicted_subject = \"dissimilar\"  # or \"unknown\"\n",
    "    \n",
    "    return predicted_subject, similarities, cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = ...  # your new EEG sample tensor here\n",
    "predicted_subject, similarity_scores = classify_sample(new_sample, subject_fingerprints)\n",
    "print(\"Predicted Subject:\", predicted_subject)\n",
    "print(\"Cosine Similarity Scores:\", similarity_scores)\n",
    "\n",
    "# --- 8. Cleanup: Remove the Hook When Done ---\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_fingerprints_new = {}\n",
    "enrollment_runs = range(1, 4)  # use runs 1, 2, 3 for enrollment\n",
    "\n",
    "for subject in range(91, 110):  # subjects 91 to 109\n",
    "    embeddings = []\n",
    "    for r in enrollment_runs:\n",
    "        file_path = f'./files/S{subject:03d}/S{subject:03d}R{r:02d}.edf'\n",
    "        try:\n",
    "            augmented = process_edf(file_path, channels, T, delta, Delta, Gamma)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "        # For each augmented sample, extract an embedding.\n",
    "        for i in range(augmented.shape[0]):\n",
    "            sample = augmented[i]  # shape: (Gamma, T, n_channels)\n",
    "            # Convert to torch.Tensor\n",
    "            sample_tensor = torch.tensor(sample, dtype=torch.float32)\n",
    "            emb = get_embedding(sample_tensor)\n",
    "            embeddings.append(emb)\n",
    "    if len(embeddings) == 0:\n",
    "        print(f\"No valid embeddings for subject {subject}\")\n",
    "        continue\n",
    "    embeddings = np.stack(embeddings, axis=0)\n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    avg_embedding = avg_embedding / np.linalg.norm(avg_embedding)\n",
    "    subject_fingerprints_new[subject] = avg_embedding\n",
    "\n",
    "print(\"Enrollment complete for subjects 91-109.\")\n",
    "\n",
    "# --- Validation ---\n",
    "# We use runs 4-6 for validation.\n",
    "validation_runs = range(4, 7)\n",
    "results = {}\n",
    "\n",
    "for subject in range(91, 110):\n",
    "    subject_results = []\n",
    "    for r in validation_runs:\n",
    "        file_path = f'./files/S{subject:03d}/S{subject:03d}R{r:02d}.edf'\n",
    "        try:\n",
    "            augmented = process_edf(file_path, channels, T, delta, Delta, Gamma)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "        for i in range(augmented.shape[0]):\n",
    "            sample = augmented[i]  # shape: (Gamma, T, n_channels)\n",
    "            sample_tensor = torch.tensor(sample, dtype=torch.float32)\n",
    "            # classify_sample returns (predicted_subject, similarities, cosine_distances)\n",
    "            predicted_subject, similarities, cosine_distances = classify_sample(sample_tensor, subject_fingerprints_new, threshold=0.275)\n",
    "            distance = cosine_distances.get(predicted_subject, None)\n",
    "            subject_results.append({\n",
    "                'run': r,\n",
    "                'aug_idx': i,\n",
    "                'predicted': predicted_subject,\n",
    "                'distance': distance\n",
    "            })\n",
    "            print(f\"Subject {subject:03d} Run {r:02d} Aug {i}: predicted = {predicted_subject}, cosine distance = {distance:.4f}\")\n",
    "    results[subject] = subject_results\n",
    "\n",
    "# --- Optional Accuracy Calculation ---\n",
    "# If the ground truth for validation is that the enrolled subject should match, compute accuracy.\n",
    "correct = 0\n",
    "total = 0\n",
    "for subject in results:\n",
    "    for res in results[subject]:\n",
    "        total += 1\n",
    "        if res['predicted'] == subject:\n",
    "            correct += 1\n",
    "\n",
    "print(f\"Validation Accuracy: {correct/total:.2%} ({correct}/{total})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
